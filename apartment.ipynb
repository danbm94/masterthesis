{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/daniel.bustillo/Documents/thesis/2016\"\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apartment1 = pd.read_csv(\"Apt1_2016.csv\", infer_datetime_format=True, index_col=0, parse_dates=True, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=0, infer_datetime_format=True, parse_dates=True,header=None)\n",
    "    li.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for home in range(0, len(li)):\n",
    "    li[home] = li[home].resample(\"1H\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names= []\n",
    "list_of_names=os.listdir(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "dataframes_list = []\n",
    "  \n",
    "# append datasets into teh list\n",
    "for i in range(len(all_files)):\n",
    "    temp_df = pd.read_csv(all_files[i], parse_dates=True, index_col=0, infer_datetime_format=True, header=None)\n",
    "    dataframes_list.append(temp_df)\n",
    "dataframes_list = [df.resample(\"1H\").mean() for df in dataframes_list]\n",
    "\n",
    "apts= pd.concat(dataframes_list, axis=1)\n",
    "apts.columns = np.arange(len(apts.columns))\n",
    "apts= apts.add_prefix(\"apt_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "apts = apts.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(time_series, timesteps):\n",
    "    dataX, dataY = [], []\n",
    "    \n",
    "    for i in range(0, len(time_series)-timesteps ): # The last \n",
    "        x = time_series[i:(i+timesteps)]    ####### Remember Python's x[a, b-1]\n",
    "        dataX.append(x)\n",
    "        y = time_series[(i+timesteps)]      ##### Remember Python's x[b]\n",
    "        dataY.append(y)\n",
    "           \n",
    "    return np.array(dataX), np.array(dataY) #dont forget to convert to np.arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(home: pd.Series, train_size=0.8, timesteps= 1, epochs=50, batch_size= 64, neurons=128):\n",
    "    \"\"\"function that wraps all the processes of data preparation and model configuration\"\"\"\n",
    "     #Drop nan values\n",
    "    home= home.dropna()\n",
    "\n",
    "    # train, test split\n",
    "    # split into train and test sets\n",
    "    values = home.values\n",
    "    values= values.reshape(len(values),1)\n",
    "    train, test = train_test_split(values, train_size=train_size, shuffle=False)\n",
    "\n",
    "    # Scaling the data to the interval [0,1]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "\n",
    "    train = scaler.transform(train)\n",
    "    test  = scaler.transform(test)\n",
    "    \n",
    "    \n",
    "    # The LSTM input layer must be 3D\n",
    "    # Create the dataset with rolling window for the training set and test set\n",
    "    X_train, y_train  = create_dataset(train, timesteps)  # lookback\n",
    "    X_test, y_test    = create_dataset(test, timesteps)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    #Define the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = \"adam\", metrics=['mse'])\n",
    "\n",
    "    #Fit the data\n",
    "    history= model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "    \n",
    "\n",
    "\n",
    "    def rmse(y_true, y_score):\n",
    "        error = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_score))\n",
    "        return error\n",
    "\n",
    "    pred_train[f\"true_{home.name}\"] = scaler.inverse_transform(y_train).flatten()  # store as 1d array\n",
    "    pred_test[f\"true_{home.name}\"]  = scaler.inverse_transform(y_test).flatten() \n",
    "\n",
    "    pred_train[f\"simple_lstm_{home.name}\"] = scaler.inverse_transform(\n",
    "                                model.predict(X_train,verbose=0)).flatten()\n",
    "\n",
    "    pred_test[f'simple_lstm_{home.name}'] = scaler.inverse_transform(model.predict(X_test)).flatten()\n",
    "\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #plot the predictions\n",
    "    plt.plot(pred_test[f'true_{home.name}'])\n",
    "    plt.plot(pred_test[f'simple_lstm_{home.name}'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    rmse_test[f'single_{home.name}'] = rmse(pred_test[f'true_{home.name}'], pred_test[f'simple_lstm_{home.name}'])\n",
    "    rmse_train[f'single_{home.name}'] = rmse(pred_train[f'true_{home.name}'], pred_train[f'simple_lstm_{home.name}'])\n",
    "\n",
    "  \n",
    "    print(f\"The RMSE in the train set is: {rmse_train[f'single_{home.name}']}\")\n",
    "    print(f\"The RMSE in the test set is: {rmse_test[f'single_{home.name}']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conv_model(home: pd.Series, train_size, timesteps= 1, n_features=1, epochs=50, batch_size= 64, plot=False):\n",
    "    \"\"\"function that wraps all the processes of data preparation and model configuration\"\"\"\n",
    "     #Drop nan values\n",
    "    home= home.dropna()\n",
    "\n",
    "    # train, test split\n",
    "    # split into train and test sets\n",
    "    values = home.values\n",
    "    values= values.reshape(len(values),1)\n",
    "    train, test = train_test_split(values, train_size=train_size, shuffle=False)\n",
    "\n",
    "    # Scaling the data to the interval [0,1]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "\n",
    "    train = scaler.transform(train)\n",
    "    test  = scaler.transform(test)\n",
    "    \n",
    "    \n",
    "    # The LSTM input layer must be 3D\n",
    "    # Create the dataset with rolling window for the training set and test set\n",
    "    X_train, y_train  = create_dataset(train, timesteps)  # lookback\n",
    "    X_test, y_test    = create_dataset(test, timesteps)\n",
    "    print(X_train.shape)\n",
    "\n",
    "   \n",
    "\n",
    "    conv = Sequential()\n",
    "    conv.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(timesteps, n_features)))\n",
    "    conv.add(MaxPooling1D(pool_size=1))\n",
    "    conv.add(Flatten())\n",
    "    conv.add(Dense(50, activation='relu'))\n",
    "    conv.add(Dense(1))\n",
    "    conv.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    #Fit the data\n",
    "    history= conv.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, shuffle=False)\n",
    "    \n",
    "    # pred_train ={}\n",
    "    # pred_test = {}\n",
    "\n",
    "    # rmse_train ={}\n",
    "    # rmse_test = {}\n",
    "\n",
    "    def rmse(y_true, y_score):\n",
    "        error = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_score))\n",
    "        return error\n",
    "\n",
    "    pred_train[f\"true_{home.name}\"] = scaler.inverse_transform(y_train).flatten()  # store as 1d array\n",
    "    pred_test[f\"true_{home.name}\"]  = scaler.inverse_transform(y_test).flatten() \n",
    "\n",
    "    pred_train[f\"convlstm_{home.name}\"] = scaler.inverse_transform(\n",
    "                                conv.predict(X_train,verbose=0)).flatten()\n",
    "\n",
    "    pred_test[f'convlstm_{home.name}'] = scaler.inverse_transform(conv.predict(X_test)).flatten()\n",
    "\n",
    "    if plot==True:\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #plot the predictions\n",
    "        plt.plot(pred_test[f'true_{home.name}'])\n",
    "        plt.plot(pred_test[f'convlstm_{home.name}'])\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    rmse_test[f'conv_{home.name}'] = rmse(pred_test[f'true_{home.name}'], pred_test[f'convlstm_{home.name}'])\n",
    "    rmse_train[f'conv_{home.name}'] = rmse(pred_train[f'true_{home.name}'], pred_train[f'convlstm_{home.name}'])\n",
    "\n",
    "  \n",
    "    print(f\"The RMSE in the train set is: {rmse_train[f'conv_{home.name}']}\")\n",
    "    print(f\"The RMSE in the test set is: {rmse_test[f'conv_{home.name}']}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_train={}\n",
    "pred_test= {}\n",
    "\n",
    "rmse_train={}\n",
    "rmse_test={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the package\n",
    "import keras_tuner as kt\n",
    "import keras.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(hp.Choice('units',[24,64,128,256,512])))\n",
    "  model.add(Dense(1, activation='relu'))\n",
    "  model.compile(loss = 'mean_squared_error', optimizer = tf.keras.optimizers.Adam(hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])), metrics=['mean_squared_error'])\n",
    "\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective= 'val_mean_squared_error',\n",
    "    max_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = apts['apt_0']\n",
    "# train, test split\n",
    "# split into train and test sets\n",
    "values = home.values\n",
    "values= values.reshape(len(values),1)\n",
    "train, test = train_test_split(values, train_size=0.9, shuffle=False)\n",
    "\n",
    "# Scaling the data to the interval [0,1]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test  = scaler.transform(test)\n",
    "\n",
    "\n",
    "# The LSTM input layer must be 3D\n",
    "# Create the dataset with rolling window for the training set and test set\n",
    "X_train, y_train  = create_dataset(train, timesteps)  # lookback\n",
    "X_test, y_test    = create_dataset(test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 11s]\n",
      "val_mean_squared_error: 0.0008049007155932486\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.0007712855003774166\n",
      "Total elapsed time: 00h 03m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7fc230168700>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "Score: 0.0007712855003774166\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 24\n",
      "learning_rate: 0.001\n",
      "Score: 0.0007757717976346612\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "learning_rate: 0.001\n",
      "Score: 0.0007857651798985898\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.001\n",
      "Score: 0.0008049007155932486\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "learning_rate: 0.1\n",
      "Score: 0.0008378610946238041\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "learning_rate: 0.01\n",
      "Score: 0.012532583437860012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.01\n",
      "Score: 0.012532583437860012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.001\n",
      "Score: 0.012532583437860012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.1\n",
      "Score: 0.012532583437860012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.1\n",
      "Score: 0.012532583437860012\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = [],[]\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('home_%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('home_%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('home_%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "values= apts.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, 1, 1, dropnan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments ={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reframed:\n",
    "    for j in range(40,79):\n",
    "        apartments[str(i)] = reframed.iloc[:, np.r_[0:39,j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM:\n",
    "    @staticmethod\n",
    "    def build(neurons, timesteps =1, n_features=1):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(neurons, input_shape =(timesteps,n_features)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss = 'mean_squared_error', optimizer = \"adam\", metrics=['mse'])\n",
    "        \n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    #client_names = list(homes.columns())\n",
    "    #get the bs\n",
    "    #bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clients\n",
    "    global_count = sum([len(clients_trn_data[client_name].dropna()) for client_name in client_names])\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = len(clients_trn_data[client_name].dropna())\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def rmse(y_true, y_score):\n",
    "    error = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_score))\n",
    "    return error\n",
    "    \n",
    "def test_model(X_test, Y_test,  model, comm_round, home):\n",
    "    preds = scaler.inverse_transform(model.predict(X_test))\n",
    "    loss = rmse(scaler.inverse_transform(Y_test), preds)\n",
    "    print('comm_round: {} | loss_in_{}: {}'.format(comm_round,home,loss))\n",
    "    return loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_task(home: pd.Series, train_size= 0.9, timesteps= 1, epochs=50, batch_size= 64):\n",
    "    \"\"\"function that wraps all the processes of data preparation and model configuration\"\"\"\n",
    "     #Drop nan values\n",
    "    home= home.dropna()\n",
    "\n",
    "    # train, test split\n",
    "    # split into train and test sets\n",
    "    values = home.values\n",
    "    values= values.reshape(len(values),1)\n",
    "    train, test = train_test_split(values, train_size=train_size, shuffle=False)\n",
    "\n",
    "    # Scaling the data to the interval [0,1]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler = scaler.fit(train)\n",
    "\n",
    "    train = scaler.transform(train)\n",
    "    test  = scaler.transform(test)\n",
    "    \n",
    "    \n",
    "    # The LSTM input layer must be 3D\n",
    "    # Create the dataset with rolling window for the training set and test set\n",
    "    X_train, y_train  = create_dataset(train, timesteps)  # lookback\n",
    "    X_test, y_test    = create_dataset(test, timesteps)\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_round = 50\n",
    "    \n",
    "#create optimizer\n",
    "lr = 0.1 \n",
    "loss='mean_squared_error'\n",
    "metrics = ['mse']\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_names = apts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleLSTM()\n",
    "global_model = smlp_global.build(128, 1, 1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_train={}\n",
    "pred_test= {}\n",
    "\n",
    "rmse_train={}\n",
    "rmse_test={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #loop through each client and create new local model\n",
    "    for house in refit:\n",
    "        smlp_local = SimpleLSTM()\n",
    "        local_model = smlp_local.build(128)\n",
    "        local_model.compile(loss=loss, \n",
    "                        optimizer=optimizer, #Trying out adam\n",
    "                        metrics=metrics)\n",
    "        \n",
    "        \n",
    "    #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train, y_train, X_test, y_test= prepare_task(refit[house])\n",
    "\n",
    "\n",
    "        local_model.fit(X_train, y_train, epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(refit, house)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "        local_loss = test_model(X_test, y_test, global_model, comm_round, house)\n",
    "\n",
    "\n",
    "        pred_train[f\"true_{house}\"] = scaler.inverse_transform(y_train).flatten()  # store as 1d array\n",
    "        pred_test[f\"true_{house}\"]  = scaler.inverse_transform(y_test).flatten() \n",
    "\n",
    "        pred_train[f\"simple_lstm_{house}\"] = scaler.inverse_transform(\n",
    "                                    local_model.predict(X_train)).flatten()\n",
    "\n",
    "        pred_test[f\"simple_lstm_{house}\"] = scaler.inverse_transform(local_model.predict(X_test)).flatten()\n",
    "\n",
    "        rmse_test[f'single_{house}'] = rmse(pred_test[f'true_{house}'], pred_test[f'simple_lstm_{house}'])\n",
    "        rmse_train[f'single_{house}'] = rmse(pred_train[f'true_{house}'], pred_train[f'simple_lstm_{house}'])\n",
    "\n",
    "        # #plot the predictions\n",
    "        # plt.plot(pred_test[f'true_{home}'])\n",
    "        # plt.plot(pred_test[f'simple_lstm_{home}'])\n",
    "        # plt.show()\n",
    "\n",
    "        K.clear_session()\n",
    "    \n",
    "            \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(rmse_test, orient='index').to_csv(\"results_apartments.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43ffa8d48b9ec0e7a69f598496eaa6d654e776178a0f9614f788c0be45b030b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
